<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yi Wei</title>
  
  <meta name="author" content="Yi Wei">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yi Wei</name>
              </p>
              <p>
              I am a third-year PhD student at the <a href="http://ivg.au.tsinghua.edu.cn/">Intelligent Vision Group (IVG)</a>, Department of Automation, Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu">Jiwen Lu</a>. My research interests lie in 3D vision, computer graphics and robotics, especially focusing on 3D scene understanding and 3D reconstruction.
              </p>  
              <p>
              Prior to that, I received my Bachelor's degree from the department of Electronic Engineering, Tsinghua University. I have also spent some time at <a href="https://www.xilinx.com/applications/megatrends/machine-learning.html">DeePhi Tech (Xilinx)</a>, <a href="https://www.sensetime.com/cn">Sensetime </a>, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a>, <a href="https://www.bytedance.com/zh/">ByteDance</a> and <a href="https://www.gs-robot.com/">Gaussian Robotics</a>.
              </p>

              <p style="text-align:center">
                <a href="mailto:y-wei19@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=zh-CN&pli=1&user=NrRf7pUAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/weiyithu"> Github </a> &nbsp/&nbsp
                <a href="https://twitter.com/yiwei47615442">Twitter</a> &nbsp/&nbsp
                <a href="data/CV_Yi_Wei.pdf">Curriculum Vitae</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/profile.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2021-07:</b> Three papers (including 1 oral) are accepted to <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-03:</b> One paper on 3D scene flow estimation is accepted to <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-03:</b> One paper on weakly supervised 3D detection is accepted to <a href="http://2011.ieee-icra.org/">ICRA 2021</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/surround.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation</papertitle>
              <br>
              <strong>Yi Wei*</strong>, <a href="https://github.com/lqzhao"> Linqing Zhao</a>*, <a href="https://scholar.google.com/citations?user=LdK9scgAAAAJ&hl=en"> Wenzhao Zheng</a>, <a href="http://www.zhengzhu.net/"> Zheng Zhu</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a>Guan Huang</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://surrounddepth.ivg-research.xyz/">[Project page]</a> <a href="https://arxiv.org/abs/2109.01129">[arXiv]</a> <a href="https://github.com/weiyithu/SurroundDepth">[Code]</a>
              <br>
              <p> We propose a SurroundDepth method to incorporate the information from multiple surrounding views to predict scale-aware depth maps across cameras.</p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/lidar.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>LiDAR Distillation: Bridging the Beam-Induced Domain Gap for 3D Object Detection</papertitle>
                <br>
                <strong>Yi Wei</strong>, <a> Zibu Wei</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a href="https://www.jiaxinli.me">Jiaxin Li</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
                <br>
                <em>arXiv</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2203.14956">[arXiv]</a> <a href="https://github.com/weiyithu/LiDAR-Distillation">[Code]</a>
                <p> We propose the LiDAR Distillation to bridge the domain gap induced by different LiDAR beams for 3D object detection.</p>
              </td>
            </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/NerfingMVS.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="http://b1ueber2y.me/"> Shaohui Liu</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a href="https://github.com/thuzhaowang">Wang Zhao</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://weiyithu.github.io/NerfingMVS">[Project page]</a> <a href="https://arxiv.org/abs/2109.01129">[arXiv]</a> <a href="https://github.com/weiyithu/NerfingMVS">[Code]</a> <a href="https://youtu.be/i-b5lPnYipA">[Video]</a>
              <br>
              <p> We present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF).</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/RandomRooms.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle> RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection </papertitle>
              <br>
              <a href="https://raoyongming.github.io/">Yongming Rao</a>*,   <a href="https://liubl1217.github.io/"> Benlin Liu</a>*,  <strong> Yi Wei </strong>,
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="http://web.cs.ucla.edu/~chohsieh/"> Cho-Jui Hsieh </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2108.07794">[arXiv]</a>
              <br>
              <p> We propose to generate random layouts of a scene by
                making use of the objects in the synthetic CAD dataset and
                learn the 3D scene representation by applying object-level
                contrastive learning on two random scenes generated from
                the same set of synthetic objects. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/idn-solver.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle> A Confidence-based Iterative Solver of Depths and Surface Normals for Deep Multi-view Stereo </papertitle>
              <br>
              <a href="https://github.com/thuzhaowang">Wang Zhao</a>*,   <a href="http://b1ueber2y.me/"> Shaohui Liu</a>*,  <strong> Yi Wei </strong>,
                <a href="https://scholar.google.com/citations?user=6_cOe58AAAAJ&hl=zh-CN&oi=sra"> Hengkai Guo </a>, <a href="https://cg.cs.tsinghua.edu.cn/people/~Yongjin/Yongjin.htm"> Yong-jin Liu </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="http://b1ueber2y.me/projects/IDN-Solver/">[Project page]</a> <a href="https://arxiv.org/abs/2201.07609v1">[arXiv]</a> <a href="https://github.com/thuzhaowang/idn-solver">[Code]</a>
              <br>
              <p> We propose a novel solver that iteratively solves for per-view depth
                map and normal map by optimizing an energy potential
                based on the locally planar assumption.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PV_RAFT.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds</papertitle>
              <br>
              <strong>Yi Wei*</strong>, <a href="https://github.com/LavenderLA">Ziyi Wang</a>*, <a href="https://raoyongming.github.io/"> Yongming Rao </a>*, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2012.00987">[arXiv]</a> <a href="https://github.com/weiyithu/PV-RAFT">[Code]</a> <a href="https://youtu.be/uh49-7eT10k">[Video]</a>
              <br>
              <p></p>
              <p> We present point-voxel correlation fields for 3D scene flow estimation which migrates the high performance of RAFT and provides a solution to build structured all-pairs correlation fields for unstructured point clouds. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/FGR.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="https://github.com/sus17">Shang Su</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2105.07647">[arXiv]</a> <a href="https://github.com/weiyithu/FGR">[Code]</a> <a href="https://youtu.be/NFGhMmMCbwA">[Video]</a>
              <br>
              <p></p>
              <p> We propose a weakly supervised 3D detection method without using 3D labels, which consists of coarse 3D segmentation and 3D bounding box estimation two stages.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OptimizeMVS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Conditional Single-view Shape Generation for Multi-view Stereo Reconstruction</papertitle>
              <br>
              <strong>Yi Wei*</strong>, <a href="http://b1ueber2y.me/"> Shaohui Liu </a>*, <a href="https://github.com/thuzhaowang"> Wang Zhao </a>*, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="http://93.179.103.61/projects/OptimizeMVS/index.html">[Project]</a> <a href="https://arxiv.org/abs/1904.06699">[arXiv]</a> <a href="https://github.com/weiyithu/OptimizeMVS">[Code]</a>
              <br>
              <p></p>
              <p>we present a new perspective towards image-based shape generation. Unlike most single-view methods which are sometimes insufficient to determine a single groundtruth shape because the back part is occluded, our method levergae multi-view consistency for 3D reconstruction. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/QM.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Quantization mimic: Towards very tiny cnn for object detection</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="https://scholar.google.com/citations?user=HxS8Bx0AAAAJ&hl=zh-CN&oi=sra"> Xinyu Pan </a>, <a href="http://qinhongwei.com/academic/"> Hongwei Qin </a>, <a href="https://yan-junjie.github.io/"> Junjie Yan </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1805.02152">[arXiv]</a>
              <br>
              <p></p>
              <p>we propose a simple and general framework for training very tiny CNNs for object detection. Our method leverages the fact that mimic and quantization can facilitate each other. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TSBnet.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Two-stream binocular network: Accurate near field finger detection based on binocular images</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="https://scholar.google.com/citations?user=Fuhq1mkAAAAJ&hl=zh-CN&oi=sra"> Guijin Wang </a>, <a href="https://github.com/cairongzhang"> Cairong Zhang </a>, <a href="https://scholar.google.com/citations?user=6_cOe58AAAAJ&hl=zh-CN&oi=sra"> Hengkai Guo </a>, <a href="https://github.com/xinghaochen"> Xinghao Chen </a>, <a href="https://scholar.google.com/citations?user=3m8I0XAAAAAJ&hl=en"> Huazhong Yang </a>, 
              <br>
              <em>IEEE Visual Communications and Image Processing (<strong>VCIP</strong>)</em>, 2017 &nbsp <font color="red"><strong>(Best Student Paper Award)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1804.10160">[arXiv]</a> 
              <br>
              <p> We propose the Two-Stream Binocular Network (TSBnet) to detect fingertips from binocular images. Different with previous depth-based methods, we directly regress 3D positions of fingertip from left and right images.</p>
            </td>
          </tr>

          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2021 National Scholarship, Tsinghua University </li>
                <li style="margin: 5px;"> 2019 Beijing Outstanding Graduate</li>
                <li style="margin: 5px;"> 2018 Caixiong Scholarship</li>
                <li style="margin: 5px;"> 2018 Baogang Outstanding Scholarship</li>
                <li style="margin: 5px;"> 2017 National Scholarship, Tsinghua University </li>
                <li style="margin: 5px;"> 2017 Qualcomm Scholarship </li>
                <li style="margin: 5px;"> 2017 Sensetime Undergraduate Scholarship </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / Program Committee Member:</b>  CVPR 2022, ICCV 2021, CVPR 2021, ICIP 2021, WACV 2021, ACCV 2020, CVPR 2020, ICIP 2019
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-IP, T-MM, T-CSVT
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=L9EQlZkj5iCdWBMkJkgi98zY_ACS8WXMtmi-BflmwK8"></script>
	  </div>        
	  <br>
	    &copy; Yi Wei | Last updated: Sep 2, 2021
</center></p>
</body>

</html>
