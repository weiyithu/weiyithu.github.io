<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yi Wei</title>
  
  <meta name="author" content="Yi Wei">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yi Wei</name>
              </p>
              <p>
              I am a research engineer in Huawei, working on 3D vision and computer graphics. I obtained my Ph.D degree at the <a href="http://ivg.au.tsinghua.edu.cn/">Intelligent Vision Group (IVG)</a>, Department of Automation, Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu">Jiwen Lu</a>. My research interests lie in 3D vision, especially focusing on 3D scene understanding and 3D reconstruction. I hope my research can help the industry applications. 
              </p>  
              <p>
              Prior to that, I received my Bachelor's degree from the department of Electronic Engineering, Tsinghua University in 2019 (Ranking 6/245). I have also spent some time at <a href="https://www.xilinx.com/applications/megatrends/machine-learning.html">DeePhi Tech (Xilinx)</a>, <a href="https://www.sensetime.com/cn">Sensetime </a>, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a>, <a href="https://www.xiaopeng.com/">XPeng</a>, <a href="https://www.bytedance.com/zh/">ByteDance</a>, <a href="https://www.phigent.ai/home">PhiGent Robtics</a>, <a href="https://www.gs-robot.com/">Gaussian Robotics</a> and  <a href="http://www.apple.com">Apple</a>.
              </p>
              <p>
	       <span style="color:rgb(240,120,80);"> We are currently recruiting doctoral and master's degree students for the class of 2025. If you are interested in 3D vision or computer graphics, please feel free to contact me. </span>
	      </p>
              <p style="text-align:center">
                <a href="mailto:y-wei19@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=zh-CN&pli=1&user=NrRf7pUAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/weiyithu"> Github </a> &nbsp/&nbsp
                <a href="https://twitter.com/yiwei47615442">Twitter</a> &nbsp/&nbsp
                <a href="data/CV_Yi_Wei.pdf">Curriculum Vitae</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/profile.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" >
                <b>2024-07:</b> One paper on 3D AIGC is accepted to <a href="https://neurips.cc//">NeurIPS 2024 2024</a>.
	      <li style="margin: 5px;" >
                <b>2024-07:</b> I graduate from Tsinghua University and will join Huawei.
	      <li style="margin: 5px;" >
                <b>2024-02:</b> One paper on 3D AIGC is accepted to <a href="https://iccv2023.thecvf.com/">CVPR 2024</a>.
              <li style="margin: 5px;" >
                <b>2023-07:</b> Two papers on occupancy prediction are accepted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>.
              <li style="margin: 5px;" >
                <b>2023-07:</b> The journal version of <a href="https://github.com/weiyithu/PV-RAFT">PV-RAFT</a> is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">T-PAMI</a>.
              <li style="margin: 5px;" >
                <b>2023-04:</b> The journal version of <a href="https://weiyithu.github.io/NerfingMVS">NerfingMVS</a> is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">T-PAMI</a>.
              <li style="margin: 5px;" >
                <b>2023-03:</b> I am a recipient of the <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2023">2023 Apple Scholars in AI/ML PhD fellowship</a>.
              <li style="margin: 5px;" >
                <b>2022-09:</b> One paper on self-supervised multi-camera depth estimation is accepted to <a href="https://corl2022.org/">CoRL 2022</a>.
              <li style="margin: 5px;" >
                <b>2022-07:</b> One paper on LiDAR-based 3D object detection is accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2022-06:</b> One paper on robotic exploration is accepted to <a href="https://iros2022.org/">IROS 2022</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-07:</b> Three papers (including 1 oral) on NeRF, depth estimation and 3D pretraining are accepted to <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-03:</b> One paper on 3D scene flow estimation is accepted to <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-03:</b> One paper on weakly supervised 3D detection is accepted to <a href="http://2011.ieee-icra.org/">ICRA 2021</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

	   <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/geolrm.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D Gaussian Generation</papertitle>
              <br>
	      Chubin Zhang, Hongliang Song, <strong>Yi Wei</strong>, Yu Chen, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://andytang15.github.io/"> Yansong Tang </a>
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a https://alibaba-yuanjing-aigclab.github.io/GeoLRM/">[Project page]</a> <a href="https://arxiv.org/abs/2406.15333">[arXiv]</a> <a href="https://github.com/alibaba-yuanjing-aigclab/GeoLRM">[Code]</a> 
              <br>
              <p>We introduce the Geometry-Aware Large Reconstruction Model (GeoLRM), an approach which can predict high-quality assets with 512k Gaussians and 21 input images in only 11 GB GPU memory. </p>
            </td>
          </tr>
		
	   <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/occnerf.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OccNeRF: Advancing 3D Occupancy Prediction in LiDAR-Free Environments</papertitle>
              <br>
              Chubin Zhang*, Juncheng Yan*, <strong>Yi Wei*</strong>, <a href="https://www.jiaxinli.me">Jiaxin Li</a>, Li Liu, <a href="https://andytang15.github.io/"></a>Yansong Tang</a>, <a href="https://duanyueqi.github.io/"></a>Yueqi Duan</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://linshan-bin.github.io/OccNeRF/">[Project page]</a> <a href="https://arxiv.org/abs/2312.09243">[arXiv]</a> <a href="https://github.com/LinShan-Bin/OccNeRF">[Code]</a> 
              <br>
              <p> We propose an OccNeRF method for self-supervised multi-camera occupancy prediction, which adopts the parameterized occupancy fields, multi-frame photometric loss and open-vocabulary 2D segmentation. </p>
            </td>
          </tr>

            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/Sherpa3D.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior</papertitle>
                <br>
                <a href="https://liuff19.github.io/">Fangfu Liu</a>, Diankun Wu, <strong>Yi Wei</strong>, <a href="https://raoyongming.github.io/"> Yongming Rao </a>, <a href="https://duanyueqi.github.io/"></a>Yueqi Duan</a>
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
                <br>
                <a href="https://liuff19.github.io/Sherpa3D/">[Project page]</a> <a href="https://arxiv.org/abs/2312.06655">[arXiv]</a> <a href="https://github.com/liuff19/Sherpa3D">[Code]</a> 
                <br>
                <p> We propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously. </p>
              </td>
            </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/surroundocc.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving</papertitle>
              <br>
              <strong>Yi Wei*</strong>, <a href="https://github.com/lqzhao"> Linqing Zhao</a>*, <a href="https://scholar.google.com/citations?user=LdK9scgAAAAJ&hl=en"> Wenzhao Zheng</a>, <a href="http://www.zhengzhu.net/"> Zheng Zhu</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://weiyithu.github.io/SurroundOcc/">[Project page]</a> <a href="https://arxiv.org/abs/2303.09551">[arXiv]</a> <a href="https://github.com/weiyithu/SurroundOcc">[Code]</a> 
              <br>
              <p> We propose a SurroundOcc method to predict the volumetric occupancy with multi-camera images and generate dense occupancy ground truth with sparse LiDAR points. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/openoccupancy.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception</papertitle>
              <br>
              <a href="https://github.com/JeffWang987"> Xiaofeng Wang</a>*,  <a href="http://www.zhengzhu.net/"> Zheng Zhu</a>*,  Wenbo Xu*, <a href="https://github.com/zhangyp15"> Yunpeng Zhang</a>, <strong>Yi Wei</strong>, Xu Chi, Yun Ye, Dalong Du,  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://ieeexplore.ieee.org/author/37407464000"> Xingang Wang </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.03991.pdf">[arXiv]</a> <a href="https://github.com/JeffWang987/OpenOccupancy">[Code]</a>
              <br>
              <p> Towards a comprehensive benchmarking of surrounding perception algorithms, we propose OpenOccupancy, which is the first surrounding semantic occupancy perception benchmark. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DPV_RAFT.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>3D Point-Voxel Correlation Fields for Scene Flow Estimation</papertitle>
              <br>
              <a href="https://wangzy22.github.io/">Ziyi Wang</a>*, <strong>Yi Wei*</strong>, <a href="https://raoyongming.github.io/"> Yongming Rao </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2023 
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10178057">[Paper]</a> <a href="https://github.com/weiyithu/PV-RAFT">[Code]</a>
              <br>
              <p></p>
              <p> We propose Deformable PV-RAFT, where the Spatial Deformation deforms the voxelized neighborhood, and the Temporal Deformation controls the iterative update process. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/NerfingMVS++.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Depth-Guided Optimization of Neural Radiance Fields for Indoor Multi-View Stereo</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="http://b1ueber2y.me/"> Shaohui Liu</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2023 
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10089515">[Paper]</a> <a href="https://github.com/weiyithu/NerfingMVS">[Code]</a> 
              <br>
              <p> Beyond NerfingMVS, we further present NerfingMVS++, where a coarse-to-fine depth priors training strategy is proposed to directly utilize sparse SfM points and the uniform sampling is replaced by Gaussian sampling to boost the performance.</p>
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/lidar.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LiDAR Distillation: Bridging the Beam-Induced Domain Gap for 3D Object Detection</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a> Zibu Wei</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a href="https://www.jiaxinli.me">Jiaxin Li</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.14956">[arXiv]</a> <a href="https://github.com/weiyithu/LiDAR-Distillation">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/558773187">[中文解读]</a>
              <p> We propose the LiDAR Distillation to bridge the domain gap induced by different LiDAR beams for 3D object detection.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/surround.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation</papertitle>
              <br>
              <strong>Yi Wei*</strong>, <a href="https://github.com/lqzhao"> Linqing Zhao</a>*, <a href="https://scholar.google.com/citations?user=LdK9scgAAAAJ&hl=en"> Wenzhao Zheng</a>, <a href="http://www.zhengzhu.net/"> Zheng Zhu</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a>Guan Huang</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>Conference on Robot Learning (<strong>CoRL</strong>)</em>, 2022
              <br>
              <a href="https://surrounddepth.ivg-research.xyz/">[Project page]</a> <a href="https://arxiv.org/abs/2204.03636">[arXiv]</a> <a href="https://github.com/weiyithu/SurroundDepth">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/565494125">[中文解读]</a>
              <br>
              <p> We propose a SurroundDepth method to incorporate the information from multiple surrounding views to predict scale-aware depth maps across cameras.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/NerfingMVS.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="http://b1ueber2y.me/"> Shaohui Liu</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a href="https://github.com/thuzhaowang">Wang Zhao</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021, <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://weiyithu.github.io/NerfingMVS">[Project page]</a> <a href="https://arxiv.org/abs/2109.01129">[arXiv]</a> <a href="https://github.com/weiyithu/NerfingMVS">[Code]</a> <a href="https://youtu.be/i-b5lPnYipA">[Video]</a> <a href="https://zhuanlan.zhihu.com/p/407123751">[中文解读]</a>
              <br>
              <p> We present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF).</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/idn-solver.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle> A Confidence-based Iterative Solver of Depths and Surface Normals for Deep Multi-view Stereo </papertitle>
              <br>
              <a href="https://github.com/thuzhaowang">Wang Zhao</a>*,   <a href="http://b1ueber2y.me/"> Shaohui Liu</a>*,  <strong> Yi Wei </strong>,
                <a href="https://scholar.google.com/citations?user=6_cOe58AAAAJ&hl=zh-CN&oi=sra"> Hengkai Guo </a>, <a href="https://cg.cs.tsinghua.edu.cn/people/~Yongjin/Yongjin.htm"> Yong-jin Liu </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="http://b1ueber2y.me/projects/IDN-Solver/">[Project page]</a> <a href="https://arxiv.org/abs/2201.07609v1">[arXiv]</a> <a href="https://github.com/thuzhaowang/idn-solver">[Code]</a>
              <br>
              <p> We propose a novel solver that iteratively solves for per-view depth
                map and normal map by optimizing an energy potential
                based on the locally planar assumption.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PV_RAFT.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds</papertitle>
              <br>
              <strong>Yi Wei*</strong>, <a href="https://wangzy22.github.io/">Ziyi Wang</a>*, <a href="https://raoyongming.github.io/"> Yongming Rao </a>*, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2012.00987">[arXiv]</a> <a href="https://github.com/weiyithu/PV-RAFT">[Code]</a> <a href="https://youtu.be/uh49-7eT10k">[Video]</a>
              <br>
              <p></p>
              <p> We present point-voxel correlation fields for 3D scene flow estimation which migrates the high performance of RAFT and provides a solution to build structured all-pairs correlation fields for unstructured point clouds. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/FGR.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="https://github.com/sus17">Shang Su</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2105.07647">[arXiv]</a> <a href="https://github.com/weiyithu/FGR">[Code]</a> <a href="https://youtu.be/NFGhMmMCbwA">[Video]</a>
              <br>
              <p></p>
              <p> We propose a weakly supervised 3D detection method without using 3D labels, which consists of coarse 3D segmentation and 3D bounding box estimation two stages.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OptimizeMVS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Conditional Single-view Shape Generation for Multi-view Stereo Reconstruction</papertitle>
              <br>
              <strong>Yi Wei*</strong>, <a href="http://b1ueber2y.me/"> Shaohui Liu </a>*, <a href="https://github.com/thuzhaowang"> Wang Zhao </a>*, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="http://93.179.103.61/projects/OptimizeMVS/index.html">[Project]</a> <a href="https://arxiv.org/abs/1904.06699">[arXiv]</a> <a href="https://github.com/weiyithu/OptimizeMVS">[Code]</a>
              <br>
              <p></p>
              <p>we present a new perspective towards image-based shape generation. Unlike most single-view methods which are sometimes insufficient to determine a single groundtruth shape because the back part is occluded, our method levergae multi-view consistency for 3D reconstruction. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/QM.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Quantization mimic: Towards very tiny cnn for object detection</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="https://scholar.google.com/citations?user=HxS8Bx0AAAAJ&hl=zh-CN&oi=sra"> Xinyu Pan </a>, <a href="http://qinhongwei.com/academic/"> Hongwei Qin </a>, <a href="https://yan-junjie.github.io/"> Junjie Yan </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1805.02152">[arXiv]</a>
              <br>
              <p></p>
              <p>we propose a simple and general framework for training very tiny CNNs for object detection. Our method leverages the fact that mimic and quantization can facilitate each other. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TSBnet.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Two-stream binocular network: Accurate near field finger detection based on binocular images</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="https://scholar.google.com/citations?user=Fuhq1mkAAAAJ&hl=zh-CN&oi=sra"> Guijin Wang </a>, <a href="https://github.com/cairongzhang"> Cairong Zhang </a>, <a href="https://scholar.google.com/citations?user=6_cOe58AAAAJ&hl=zh-CN&oi=sra"> Hengkai Guo </a>, <a href="https://github.com/xinghaochen"> Xinghao Chen </a>, <a href="https://scholar.google.com/citations?user=3m8I0XAAAAAJ&hl=en"> Huazhong Yang </a>, 
              <br>
              <em>IEEE Visual Communications and Image Processing (<strong>VCIP</strong>)</em>, 2017 &nbsp <font color="red"><strong>(Best Student Paper Award)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1804.10160">[arXiv]</a> 
              <br>
              <p> We propose the Two-Stream Binocular Network (TSBnet) to detect fingertips from binocular images. Different with previous depth-based methods, we directly regress 3D positions of fingertip from left and right images.</p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experiences</heading>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/apple.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Apple</papertitle>
              <br>
              AI/ML Group, Research Intern
              <br>
              Topic: 3D AIGC
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/gaussian.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Gaussian Robotics</papertitle>
              <br>
              Gaussian-Tsinghua joint laboratory, Project leader
              <br>
              Topic: Sensor calibration, Drivable space detection, LiDAR-based 3D object detection, Depth estimation, 3D reconstruction
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/bytedance.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>ByteDance</papertitle>
              <br>
              SLAM & 3D Vision Group, Engineer&Research Intern
              <br>
              Topic: Sky AR, Advertisement AR, Self-supervised depth estimation, Plane-assisted multi-view stereo, Multiple plane detection
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/xpeng.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>XPeng</papertitle>
              <br>
              LiDAR Group, Engineer Intern
              <br>
              Topic: LiDAR-based 3D object detection, LiDAR-based model quantization
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/microsoft.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MSRA</papertitle>
              <br>
              Intelligent Multimedia Group, Research Intern
              <br>
              Topic: Multi-view hand pose estimation
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/sensetime.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Sensetime</papertitle>
              <br>
              Video Intelligence Group, Engineer&Research Intern
              <br>
              Topic: Model compression
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/deephi.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deephi</papertitle>
              <br>
              Engineer Intern
              <br>
              Topic: Real-time object detection
            </td>
          </tr>
        </table>
        

          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
		<li style="margin: 5px;"> 2024 Beijing Outstanding Graduate / 北京市优秀毕业生 </li>
		<li style="margin: 5px;"> 2023 Huawei TopMinds / 华为天才少年称号 </li>
                <li style="margin: 5px;"> 2023 Apple Scholar / 苹果学者奖学金  (22 people in the world, 2 people in China) </li>
                <li style="margin: 5px;"> 2023 Ubiquant Scholar / 九坤奖学金 </li>
                <li style="margin: 5px;"> 2021 National Scholarship / 国家奖学金 </li>
                <li style="margin: 5px;"> 2019 Beijing Outstanding Graduate / 北京市优秀毕业生 </li>
                <li style="margin: 5px;"> 2018 Caixiong Scholarship / 清华科创类专项奖 (10 people in Tsinghua) </li>
                <li style="margin: 5px;"> 2018 Baogang Outstanding Scholarship / 宝钢优秀学生特等奖 (1 person in Tsinghua) </li>
                <li style="margin: 5px;"> 2017 National Scholarship / 国家奖学金 </li>
                <li style="margin: 5px;"> 2017 Qualcomm Scholarship / 高通奖学金 (30 people in Tsinghua) </li>
                <li style="margin: 5px;"> 2017 Sensetime Scholarship / 商汤奖学金 (30 people in China) </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / Program Committee Member:</b>  CVPR 2024, ICCV 2023, ICRA 2023, ECCV 2022, CVPR 2022, ICCV 2021, CVPR 2021, ICIP 2021, WACV 2021, ACCV 2020, CVPR 2020, ICIP 2019
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-PAMI, T-IP, T-MM, T-CSVT
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=L9EQlZkj5iCdWBMkJkgi98zY_ACS8WXMtmi-BflmwK8"></script>
	  </div>        
	  <br>
	    &copy; Yi Wei | Last updated: July 3, 2024
</center></p>
</body>

</html>
