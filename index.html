<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yi Wei</title>
  
  <meta name="author" content="Yi Wei">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yi Wei</name>
              </p>
              <p>
              I am a second-year PhD student at the <a href="http://ivg.au.tsinghua.edu.cn/">Intelligent Vision Group (IVG)</a>, Department of Automation, Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu">Jiwen Lu</a>. My research interests lie in 3D vision, computer graphics and robotics, especially focusing on 3D scene understanding and 3D reconstruction.
              </p>  
              <p>
              Prior to that, I received my Bachelor's degree from the department of Electronic Engineering, Tsinghua University. I have also spent some time at <a href="https://www.xilinx.com/applications/megatrends/machine-learning.html">DeePhi Tech (Xilinx)</a>, <a href="https://www.sensetime.com/cn">Sensetime </a>, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> and <a href="https://www.bytedance.com/zh/">ByteDance</a>.
              </p>

              <p style="text-align:center">
                <a href="mailto:y-wei19@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=zh-CN&pli=1&user=NrRf7pUAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/weiyithu"> Github </a> &nbsp/&nbsp
                <a href="https://twitter.com/yiwei47615442">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/profile.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2021-03:</b> One paper on 3D scene flow estimation is accepted to <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-03:</b> One paper on weakly supervised 3D detection is accepted to <a href="http://2011.ieee-icra.org/">ICRA 2021</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PV_RAFT.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds</papertitle>
              <br>
              <strong>Yi Wei*</strong>, <a href="https://github.com/LavenderLA">Ziyi Wang</a>*, <a href="https://raoyongming.github.io/"> Yongming Rao </a>*, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2012.00987">[arXiv]</a> <a href="https://github.com/weiyithu/PV-RAFT">[Code]</a> <a href="https://youtu.be/uh49-7eT10k">[Video]</a>
              <br>
              <p></p>
              <p> We present point-voxel correlation fields for 3D scene flow estimation which migrates the high performance of RAFT and provides a solution to build structured all-pairs correlation fields for unstructured point clouds. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/FGR.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="https://github.com/sus17">Shang Su</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2105.07647">[arXiv]</a> <a href="https://github.com/weiyithu/FGR">[Code]</a> <a href="https://youtu.be/NFGhMmMCbwA">[Video]</a>
              <br>
              <p></p>
              <p> We propose a weakly supervised 3D detection method without using 3D labels, which consists of coarse 3D segmentation and 3D bounding box estimation two stages.
            </td>
          </tr>

        
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/seggroup.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SegGroup: Seg-Level Supervision for 3D Instance and Semantic Segmentation</papertitle>
              <br>
              <a href="https://www.antao.site/"> An Tao </a>, <a href="https://geometry.stanford.edu/member/duanyq19/index.html"> Yueqi Duan </a>, <strong>Yi Wei</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>arXiv, 2020</em>
              <br>
              <a href="https://arxiv.org/abs/2012.10217">[arXiv]</a> <a href="https://github.com/AnTao97/SegGroup">[Code]</a>
              <br>
              <p></p>
              <p>In this paper, we discover that the locations of instances matter for 3D scene segmentation. By fully taking the advantages of locations, we design a weakly supervised point cloud segmentation algorithm that only requires clicking on one point per instance to indicate its location for annotation. With over-segmentation for pre-processing, we extend these location annotations into segments as seg-level labels. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GIN.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Graph Interaction Networks for Relation Transfer in Human Activity Videos</papertitle>
              <br>
              <a href="https://andytang15.github.io/"> Yansong Tang </a>, <strong>Yi Wei</strong>, <a href="https://github.com/yuxumin"> Xumin Yu </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2020
              <br>
              <a href="data/GIN.pdf">[PDF]</a> <a>[code] (to come)</a>
              <br>
              <p></p>
              <p>We propose a graph interaction networks (GINs) model for transferring relation knowledge across two graphs two different scenarios for video analysis, including a new proposed setting for unsupervised skeleton-based action recognition across different datasets, and supervised group activity recognition with multi-modal inputs. </p>
            </td>
          </tr>
         
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OptimizeMVS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Conditional Single-view Shape Generation for Multi-view Stereo Reconstruction</papertitle>
              <br>
              <strong>Yi Wei*</strong>, <a href="http://b1ueber2y.me/"> Shaohui Liu </a>*, <a href="https://github.com/thuzhaowang"> Wang Zhao </a>*, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="http://93.179.103.61/projects/OptimizeMVS/index.html">[Project]</a> <a href="https://arxiv.org/abs/1904.06699">[arXiv]</a> <a href="https://github.com/weiyithu/OptimizeMVS">[Code]</a>
              <br>
              <p></p>
              <p>we present a new perspective towards image-based shape generation. Unlike most single-view methods which are sometimes insufficient to determine a single groundtruth shape because the back part is occluded, our method levergae multi-view consistency for 3D reconstruction. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/QM.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Quantization mimic: Towards very tiny cnn for object detection</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="https://scholar.google.com/citations?user=HxS8Bx0AAAAJ&hl=zh-CN&oi=sra"> Xinyu Pan </a>, <a href="http://qinhongwei.com/academic/"> Hongwei Qin </a>, <a href="https://yan-junjie.github.io/"> Junjie Yan </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1805.02152">[arXiv]</a>
              <br>
              <p></p>
              <p>we propose a simple and general framework for training very tiny CNNs for object detection. Our method leverages the fact that mimic and quantization can facilitate each other. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TSBnet.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Two-stream binocular network: Accurate near field finger detection based on binocular images</papertitle>
              <br>
              <strong>Yi Wei</strong>, <a href="https://scholar.google.com/citations?user=Fuhq1mkAAAAJ&hl=zh-CN&oi=sra"> Guijin Wang </a>, <a href="https://github.com/cairongzhang"> Cairong Zhang </a>, <a href="https://scholar.google.com/citations?user=6_cOe58AAAAJ&hl=zh-CN&oi=sra"> Hengkai Guo </a>, <a href="https://github.com/xinghaochen"> Xinghao Chen </a>, <a href="https://scholar.google.com/citations?user=3m8I0XAAAAAJ&hl=en"> Huazhong Yang </a>, 
              <br>
              <em>IEEE Visual Communications and Image Processing (<strong>VCIP</strong>)</em>, 2017 &nbsp <font color="red"><strong>(Best Student Paper Award)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1804.10160">[arXiv]</a> 
              <br>
              <p> We propose the Two-Stream Binocular Network (TSBnet) to detect fingertips from binocular images. Different with previous depth-based methods, we directly regress 3D positions of fingertip from left and right images.</p>
            </td>
          </tr>

          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2019 Beijing Outstanding Graduate</li>
                <li style="margin: 5px;"> 2018 Caixiong Scholarship</li>
                <li style="margin: 5px;"> 2018 Baogang Outstanding Scholarship</li>
                <li style="margin: 5px;"> 2017 National Scholarship, Tsinghua University </li>
                <li style="margin: 5px;"> 2017 Qualcomm Scholarship </li>
                <li style="margin: 5px;"> 2017 Sensetime Undergraduate Scholarship </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / Program Committee Member:</b> ICCV 2021, CVPR 2021, ICIP 2021, WACV 2021, ACCV 2020, CVPR 2020, ICIP 2019
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-IP, T-MM, T-CSVT
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=L9EQlZkj5iCdWBMkJkgi98zY_ACS8WXMtmi-BflmwK8"></script>
	  </div>        
	  <br>
	    &copy; Yi Wei | Last updated: June 16, 2021
</center></p>
</body>

</html>
